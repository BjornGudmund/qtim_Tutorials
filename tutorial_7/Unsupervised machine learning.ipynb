{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised machine learning\n",
    "\n",
    "Previously, we have taken MR images and attempt assign to them values of normal/diseased, at a whole image level (classification) or a pixel level (segmentation). But what if we don't have labels?\n",
    "\n",
    "We are going to spent this tutorial exploring how to find patterns in data using unsupervised machine learning. We'll introduce three new tools that require no labels (though we'll keep track of them to evaluate our performance):\n",
    "\n",
    "- Autoencoders: deep neural network designed minimize the \"reconstruction error\" between the input and output (which are the same)\n",
    "- K-means clustering: a way of automatically find groups of unlabelled data points in space based on distances between them\n",
    "- K-nearest neighbours: assigning a new data point a label based on its proximity to other labelled data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "train_path = 'C:/Users/jxb29/Dropbox (Partners HealthCare)/Teaching/BRATS_10_Updated/*/*.nii.gz'\n",
    "sequences = ['t1', 't2', 't1ce', 'flair']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.measure import label, regionprops\n",
    "\n",
    "def normalize_images(channel_copy):\n",
    "        \n",
    "    label_image = label(channel_copy == 0)\n",
    "\n",
    "    largest_label, largest_area = None, 0\n",
    "    for region in regionprops(label_image):\n",
    "        if region.area > largest_area:\n",
    "            largest_area = region.area\n",
    "            largest_label = region.label\n",
    "\n",
    "    mask = label_image == largest_label     \n",
    "    masked_channel = np.ma.masked_where(mask, channel_copy)\n",
    "\n",
    "    masked_channel = masked_channel - np.mean(masked_channel)\n",
    "    masked_channel = masked_channel / np.std(masked_channel)\n",
    "    masked_channel = np.ma.getdata(masked_channel)\n",
    "    return masked_channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import nibabel as nib\n",
    "from os.path import basename, join\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "all_images = glob(train_path)\n",
    "\n",
    "slices = []\n",
    "labels = []\n",
    "\n",
    "thumb_data = []\n",
    "\n",
    "no_slices = 40\n",
    "\n",
    "for nifti_file in all_images:\n",
    "    \n",
    "    seq = basename(nifti_file).split('.')[0].split('_')[-1]\n",
    "    \n",
    "    if seq not in sequences:\n",
    "        continue\n",
    "    \n",
    "    # Load Nifti file, normalize it\n",
    "    vol = nib.load(nifti_file).get_data()\n",
    "    vol = normalize_images(vol)\n",
    "    \n",
    "    # Take a middle-ish section of the volume\n",
    "    halfway_point = vol.shape[2] // 2\n",
    "    sample = [vol[:,:,i] for i in range(halfway_point-(no_slices//2), halfway_point+(no_slices//2))]\n",
    "    slices.extend(sample)\n",
    "    \n",
    "    # Generate thumbnails\n",
    "    for i, np_arr in enumerate(sample):\n",
    "        \n",
    "        pil_img = Image.fromarray(np_arr).resize((100, 100))\n",
    "        #file_name = basename(nifti_file).split('.')[0] + '_' + str(i) + '.tif'\n",
    "        file_name = join('thumbnails', basename(nifti_file).split('.')[0] + f'_{i}.tif')\n",
    "        pil_img.save(file_name)\n",
    "        \n",
    "        thumb_dict = dict(file_name=file_name, subject_name=basename(nifti_file).split('.')[0], sequence=seq)\n",
    "        thumb_data.append(thumb_dict)\n",
    "        \n",
    "    # Keep track of the labels (sequence ID: 0 == t1, 1 == t2)\n",
    "    index = sequences.index(seq)\n",
    "    index_list = [index] * no_slices\n",
    "    labels.extend(index_list)\n",
    "    \n",
    "    \n",
    "df = pd.DataFrame(data=thumb_data)\n",
    "print(df)\n",
    "df.to_csv('thumbs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (samples: 40 * N, rows: 240, columns: 240, channels: 1)\n",
    "X = np.expand_dims(np.asarray(slices), axis=-1)\n",
    "y = np.asarray(labels)\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "from keras.utils.io_utils import HDF5Matrix\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "\n",
    "def save_hdf5_file(train_data, output_filename):\n",
    "    \n",
    "    with h5py.File(output_filename, 'w') as file_handle:\n",
    "        file_handle.create_dataset('train', data=train_data, dtype=train_data.dtype)\n",
    "\n",
    "class ReconGenerator:\n",
    "  \n",
    "    def __init__(self, save_file, training_data=None, batch_size=20, augmentation=None):\n",
    "    \n",
    "        self.training_data = training_data\n",
    "        self.save_file = save_file\n",
    "        self.batch_size = batch_size\n",
    "        self.augmentation = augmentation\n",
    "        self.seed = 1989\n",
    "        \n",
    "        if training_data is not None:\n",
    "            save_hdf5_file(training_data, self.save_file)\n",
    "            \n",
    "        self.X_train = HDF5Matrix(self.save_file, 'train')\n",
    "        self.image_shape = self.X_train.shape[1:]\n",
    "        self.steps = self.X_train.shape[0] // self.batch_size\n",
    "\n",
    "    def generate(self):\n",
    "\n",
    "        aug_dict = dict()\n",
    "        if self.augmentation is not None:\n",
    "            aug_dict = self.augmentation\n",
    "\n",
    "        X_datagen = ImageDataGenerator(**aug_dict)\n",
    "        X_generator = X_datagen.flow(self.X_train, seed=self.seed, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "        return zip(X_generator, X_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "recon_gen = ReconGenerator('ae_data.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "import models\n",
    "ae = models.autoencoder(image_shape=recon_gen.image_shape)\n",
    "ae.compile(loss='mse', optimizer='sgd', metrics=['mae'])\n",
    "ae.fit_generator(recon_gen.generate(), epochs=10, steps_per_epoch=recon_gen.steps)\n",
    "ae.save_weights('ae_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import models\n",
    "encoder = models.autoencoder(image_shape=recon_gen.image_shape, encoder_only=True)\n",
    "encoder.load_weights('ae_weights.h5', by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = encoder.predict(recon_gen.X_train)\n",
    "np.savez('features.npz', features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.load('features.npz')['arr_0']  # bit weird, but necessary\n",
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(n_components=2, verbose=1)\n",
    "T = tsne.fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(T.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set_style('white')\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def plot_embedding(T, y):\n",
    "    plt.figure(figsize=(10,10))\n",
    "\n",
    "    for c, i in {'r': 0, 'b': 1, 'c': 2, 'm': 3}.items():\n",
    "\n",
    "        idx = y == i\n",
    "        plt.scatter(T[idx, 0], T[idx, 1], c=c, marker='.', s=10, alpha=.5, label=sequences[i].upper())\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install -c conda-forge umap-learn\n",
    "import umap\n",
    "embedding = umap.UMAP().fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('thumbs.csv', index_col=0)\n",
    "df['x'] = embedding[:, 0]\n",
    "df['y'] = embedding[:, 1]\n",
    "df['color'] = df['sequence'].replace({'t1': '#c866d1', 't2': '#6674d1', 't1ce': '#66d171', 'flair': '#f4b942'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('thumbs.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plan for today\n",
    "\n",
    "* Review last week\n",
    "* Demonstrate Bokeh plots\n",
    "* Create some thumbnail versions of our data\n",
    "* Create an interactive visualization in Bokeh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('thumbs.csv', index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.plotting import figure, output_file, show, ColumnDataSource\n",
    "\n",
    "output_file(\"bokeh.html\")\n",
    "\n",
    "source = ColumnDataSource(data=dict(\n",
    "    x=df['x'],\n",
    "    y=df['y'],\n",
    "    desc=df['subject_name'],\n",
    "    imgs=df['file_name'],\n",
    "    color=df['color']\n",
    "))\n",
    "\n",
    "TOOLTIPS = \"\"\"\n",
    "    <div>\n",
    "        <div>\n",
    "            <img\n",
    "                src=\"@imgs\" height=\"100\" alt=\"@imgs\" width=\"100\"\n",
    "                style=\"float: left; margin: 0px 15px 15px 0px;\"\n",
    "                border=\"2\"\n",
    "            ></img>\n",
    "        </div>\n",
    "        <div>\n",
    "            <span style=\"font-size: 17px; font-weight: bold;\">@desc</span>\n",
    "            <span style=\"font-size: 15px; color: #966;\">[$index]</span>\n",
    "        </div>\n",
    "        <div>\n",
    "            <span style=\"font-size: 15px;\">Location</span>\n",
    "            <span style=\"font-size: 10px; color: #696;\">($x, $y)</span>\n",
    "        </div>\n",
    "    </div>\n",
    "\"\"\"\n",
    "\n",
    "p = figure(plot_width=800, plot_height=800, tooltips=TOOLTIPS,\n",
    "           title=\"UMAP applied to autoencoded MR features\")\n",
    "\n",
    "p.circle('x', 'y', fill_color='color', fill_alpha=0.5, line_alpha=0., size=8, source=source)\n",
    "\n",
    "show(p)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
